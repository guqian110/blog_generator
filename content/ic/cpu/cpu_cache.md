Title: CPU 关键技术 —— Cache
Date: 2021-01-13 19:21
Category: IC
Tags: CPU, Cache
Slug: cpu_cache
Author: Qian Gu
Series: CPU 关键技术
Status: draft
Summary: 总结 Cache 的设计细节

## 为什么需要 Cache？

### 面临的问题

众所周知，处理器的集成度一直在按照摩尔定律逐渐提高，在 20 世纪的后 20 年内，core 的时钟频率也基本按照每 18 个月翻一番的速度增长，但是 DRAM 的时钟频率增长速度却每年只有 7%，所以两者的速度之间就形成了一个“剪刀差”：随着时间的推移，这个差距会越来越大，即使在现在这个多核的时代，core 的频率不再怎么提升了，这个差距仍然在扩大，这是因为多核对内存的带宽需求也相应增加了。

换个说法，这是码农和硅农之间的矛盾：码农希望存储器的容量尽可能大的同时速度足够快，而实际上硅农受限于半导体技术，速度快的 SRAM 容量无法做得很大，成本太高；而容量大的 DRAM 速度无法做得很快。举个例子，假设一个 core 的频率为 2 GHz 的 4-way 超标量处理器，它直接访问 latency = 100ns 的 DRAM，那么访问一次 DRAM 的时间内处理器可以执行多少条指令呢？

$$4*100*10^-9*2*10^9 = 800$$

这显然是不可接受的。

### 解决方案

按照传统的冯·诺依曼结构，指令和数据都在内存中，CPU 只负责处理，内存和 CPU 关系就像是仓库和工厂，工厂加工的原料和生产的产品都要放在仓库中。但是摩尔定律和“剪刀差”导致工厂和仓库之间的运输能力成为整个系统的瓶颈，最简单直观的解决办法就是在工厂里面做一个小仓库，对应在处理器中就是存储器层次 `Memory Hierarchy`。 

cache 能缓解问题的原因在于程序具有**局部性原理**：

+ `时间局部性`：一个数据被访问之后，短期内很大概率会被再次访问
+ `空间局部性`：一个数据被访问之后，短期内很大概率会访问相邻数据

Cache 的出现可以说是一种无奈的妥协。如果 DRAM 的速度足够快，或者 SRAM 的容量可以做到足够大，我们的烦恼不复存在了，cache 也没有存在的必要了。但是在未来一段时间内，当今硅工艺不发生革命性变化的前提下，这是很难实现的一件事情，所以 cache 是有必要的。

## 如何设计 Cache？

cache 的设计可以总结为下面几个问题。

### 容量选择

经验 + 微架构，L1 + L2

L1 的目标：

L2 的目标：

### 映射方式

cache 的工作方式和停车场非常类似，如果停车场（cache）中有可用的空车位（cache line），那么汽车（数据）就可以停在该车位中；如果停车场已经没有空车位，那么就要先把某个车开出来（数据替换出去），然后才能把新来的车停进去。而在停车场找车时，如果停车场很大，而且所有的车都随机停，那么找车（查找数据）的速度就会很慢。

| 类型     | 类比         | 优点                    | 缺点                 |
| ------- | ----------- | ----------------------- | ------------------- |
| 直接映射 | 固定车位      | 硬件简单、成本低，查找速度快 | 不灵活、易冲突、利用率低 |
| 全相联   | 随机车位      | 冲突小、利用率高          | 硬件复杂，查找速度慢    |
| 组相联   | 区域内随机车位 | 折中                    | 折中                 |

组相联是另外两种方式的折中：组之间是直接映射、组内是全相联。直接映射可以看作是组数 set = full 的特例，全相联可以看作是 set = 1 的特例。

### 替换策略

### 写回方式

## Cache 的规格

#### block size

较大的 block 的可以更好地利用空间局部性，所以可以降低 miss rate，但是当 block 和 cache 容量的比例大到一定程度时，因为 block 的数量变得很少，此时会有大量的冲突，数据在被再次访问前就已经被替换出去了，而且太大的 block 内部数据的空间局部性也会降低，所以导致 miss rate 反而上升。

随着 block 的增大，miss rate 的改善逐渐降低，但是在不改变 memory 系统的前提下，miss penalty 会随着 block 的增大而增大，所以当 miss penalty 超过了 miss rate 的收益，cache 的性能就会变低。

!!! tip
    较大 block 会导致较长的传输时间，虽然这部分时间很难优化，但是我们可以隐藏一些数据传输的时间，从而降低 miss penalty。实现这个效果的最简单的技术叫做 `early restart`：一旦接受到需要的 word 就立即就开始执行，而不是等到整个 block 都返回后才开始执行。许多处理器都在 I-cache 上使用这个技术，而且效果甚佳，这是因为大部分指令访问都具有连续性。这个技术对于 D-cache 来说效果就没那么好了，因为数据访问的预测性没那么好，在传输结束前请求另外一个 block 中 word 的概率很高，而此时前一次请求的数据传输还没有结束，所以仍然会导致处理器 stall。

    还有一种更加复杂的机制叫做 `requested word first` 或者是 `critical word first`，这种方案会重新组织 memory 的结构，使得被请求的 word 优先返回，然后按照顺序返回后续数据，最后反卷到 block 的开头部分。这种方法比 early restart 稍微ie快一点，但是会受到相同的限制。